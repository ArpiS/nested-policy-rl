{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountaincar Environment\n",
    "* Eval can start anywhere from left to goal state, vel 0 (also training). They need 71 episodes\n",
    "* Modify cartpole to only have two actions-> left and right. The magnitude of the actions are much larger in nfq paper\n",
    "* Hint to goal, which sometimes makes the agent perform worse\n",
    "* Group: the magnitude of the action\n",
    "* Made the forces symmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configargparse\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from environments import MountainCarEnv, Continuous_MountainCarEnv\n",
    "from models.agents import NFQAgent\n",
    "from models.networks import NFQNetwork, ContrastiveNFQNetwork\n",
    "from util import get_logger, close_logger, load_models, make_reproducible, save_models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(init_experience=100, bg_only=False, continuous=False, agent=None, dataset='train'):\n",
    "    if continuous:\n",
    "        env_bg = Continuous_MountainCarEnv(group=0)\n",
    "        env_fg = Continuous_MountainCarEnv(group=1)\n",
    "    else:\n",
    "        env_bg = MountainCarEnv(group=0)\n",
    "        env_fg = MountainCarEnv(group=1)\n",
    "    bg_rollouts = []\n",
    "    fg_rollouts = []\n",
    "    if init_experience > 0:\n",
    "        for _ in range(init_experience):\n",
    "            rollout_bg, episode_cost = env_bg.generate_rollout(\n",
    "                agent, render=False, group=0, dataset=dataset\n",
    "            )\n",
    "            bg_rollouts.extend(rollout_bg)\n",
    "            if not bg_only:\n",
    "                rollout_fg, episode_cost = env_fg.generate_rollout(\n",
    "                    agent, render=False, group=1, dataset=dataset\n",
    "                )\n",
    "                fg_rollouts.extend(rollout_fg)\n",
    "    bg_rollouts.extend(fg_rollouts)\n",
    "    all_rollouts = bg_rollouts.copy()\n",
    "    return all_rollouts, env_bg, env_fg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Mountaincar\n",
    "* There are quite a few actions. This makes it hard for FQI to learn to succeed on this task. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "is_contrastive=False\n",
    "epoch = 400\n",
    "evaluations = 10\n",
    "verbose=True\n",
    "print(\"Generating Data\")\n",
    "train_rollouts, train_env_bg, train_env_fg = generate_data()\n",
    "test_rollouts, eval_env_bg, eval_env_fg = generate_data()\n",
    "\n",
    "nfq_net = ContrastiveNFQNetwork(\n",
    "    state_dim=train_env_bg.state_dim, is_contrastive=is_contrastive\n",
    ")\n",
    "optimizer = optim.Adam(nfq_net.parameters(), lr=1e-1)\n",
    "\n",
    "nfq_agent = NFQAgent(nfq_net, optimizer)\n",
    "\n",
    "# NFQ Main loop\n",
    "bg_success_queue = [0] * 3\n",
    "fg_success_queue = [0] * 3\n",
    "epochs_fg = 0\n",
    "eval_fg = 0\n",
    "for k, epoch in enumerate(tqdm.tqdm(range(epoch + 1))):\n",
    "    state_action_b, target_q_values, groups = nfq_agent.generate_pattern_set(\n",
    "        train_rollouts\n",
    "    )\n",
    "    X = state_action_b\n",
    "    train_groups = groups\n",
    "\n",
    "    if not nfq_net.freeze_shared:\n",
    "        loss = nfq_agent.train((state_action_b, target_q_values, groups))\n",
    "\n",
    "    eval_episode_length_fg, eval_success_fg, eval_episode_cost_fg = 0, 0, 0\n",
    "    if nfq_net.freeze_shared:\n",
    "        eval_fg += 1\n",
    "\n",
    "        if eval_fg > 50:\n",
    "            loss = nfq_agent.train((state_action_b, target_q_values, groups))\n",
    "\n",
    "    (eval_episode_length_bg, eval_success_bg, eval_episode_cost_bg) = nfq_agent.evaluate_car(eval_env_bg, render=False)\n",
    "    (eval_episode_length_fg,eval_success_fg, eval_episode_cost_fg) = nfq_agent.evaluate_car(eval_env_fg, render=False)\n",
    "\n",
    "    bg_success_queue = bg_success_queue[1:]\n",
    "    bg_success_queue.append(1 if eval_success_bg else 0)\n",
    "\n",
    "    fg_success_queue = fg_success_queue[1:]\n",
    "    fg_success_queue.append(1 if eval_success_fg else 0)\n",
    "\n",
    "    printed_bg = False\n",
    "    printed_fg = False\n",
    "\n",
    "    if sum(bg_success_queue) == 3 and not nfq_net.freeze_shared == True:\n",
    "        if epochs_fg == 0:\n",
    "            epochs_fg = epoch\n",
    "        printed_bg = True\n",
    "        nfq_net.freeze_shared = True\n",
    "        if verbose:\n",
    "            print(\"FREEZING SHARED\")\n",
    "        if is_contrastive:\n",
    "            for param in nfq_net.layers_shared.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in nfq_net.layers_last_shared.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in nfq_net.layers_fg.parameters():\n",
    "                param.requires_grad = True\n",
    "            for param in nfq_net.layers_last_fg.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            for param in nfq_net.layers_fg.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in nfq_net.layers_last_fg.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            optimizer = optim.Adam(\n",
    "                itertools.chain(\n",
    "                    nfq_net.layers_fg.parameters(),\n",
    "                    nfq_net.layers_last_fg.parameters(),\n",
    "                ),\n",
    "                lr=1e-1,\n",
    "            )\n",
    "            nfq_agent._optimizer = optimizer\n",
    "\n",
    "    if sum(fg_success_queue) == 3:\n",
    "        printed_fg = True\n",
    "        break\n",
    "\n",
    "eval_env_bg.step_number = 0\n",
    "eval_env_fg.step_number = 0\n",
    "\n",
    "eval_env_bg.max_steps = 1000\n",
    "eval_env_fg.max_steps = 1000\n",
    "\n",
    "performance_fg = []\n",
    "performance_bg = []\n",
    "num_steps_bg = []\n",
    "num_steps_fg = []\n",
    "total = 0\n",
    "for it in range(evaluations):\n",
    "    (\n",
    "        eval_episode_length_bg,\n",
    "        eval_success_bg,\n",
    "        eval_episode_cost_bg,\n",
    "    ) = nfq_agent.evaluate_car(eval_env_bg, False)\n",
    "    if verbose:\n",
    "        print(eval_episode_length_bg, eval_success_bg, eval_episode_cost_bg)\n",
    "    num_steps_bg.append(eval_episode_length_bg)\n",
    "    performance_bg.append(eval_episode_length_bg)\n",
    "    total += 1\n",
    "    train_env_bg.close()\n",
    "    eval_env_bg.close()\n",
    "\n",
    "    (\n",
    "        eval_episode_length_fg,\n",
    "        eval_success_fg,\n",
    "        eval_episode_cost_fg,\n",
    "    ) = nfq_agent.evaluate_car(eval_env_fg, False)\n",
    "    if verbose:\n",
    "        print(eval_episode_length_fg, eval_success_fg, eval_episode_cost_fg)\n",
    "    num_steps_fg.append(eval_episode_length_fg)\n",
    "    performance_fg.append(eval_episode_length_fg)\n",
    "    total += 1\n",
    "    train_env_fg.close()\n",
    "    eval_env_fg.close()\n",
    "print(\"Fg trained after \" + str(epochs_fg) + \" epochs\")\n",
    "print(\"BG stayed up for steps: \", num_steps_bg)\n",
    "print(\"FG stayed up for steps: \", num_steps_fg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Mountaincar\n",
    "* Has three actions, easier for FQI to understand what's going on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using random actions to generate rollout results in the agent never succeeding"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_rollouts, train_env_bg, train_env_fg = generate_data(bg_only=True, continuous=False)\n",
    "rewards = [r[2] for r in train_rollouts]\n",
    "actions = [r[1] for r in train_rollouts]\n",
    "sns.distplot(rewards, bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a model to generate actions that are better than random\n",
    "* This doesn't do well either without any adjustments. The model can't really learn how to make it past -0.4 position\n",
    "* Even if we change the reward to be continuous, FQI can't learn it.\n",
    "* However, we can make the reset speed 0.3, we sometimes learn it. \n",
    "* When we train a model, the reset has a speed of 0. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "is_contrastive=False\n",
    "epoch = 100\n",
    "\n",
    "train_rollouts, train_env_bg, train_env_fg = generate_data(bg_only=True, continuous=False, dataset='train')\n",
    "test_rollouts, eval_env_bg, eval_env_fg = generate_data(bg_only=True, continuous=False, dataset='train')\n",
    "nfq_net = ContrastiveNFQNetwork(\n",
    "    state_dim=train_env_bg.state_dim, is_contrastive=is_contrastive, deep=True\n",
    ")\n",
    "optimizer = optim.Adam(nfq_net.parameters(), lr=1e-1)\n",
    "\n",
    "nfq_agent = NFQAgent(nfq_net, optimizer)\n",
    "\n",
    "# NFQ Main loop\n",
    "bg_success_queue = [0] * 3\n",
    "fg_success_queue = [0] * 3\n",
    "epochs_fg = 0\n",
    "eval_fg = 0\n",
    "train_rewards = [r[2] for r in train_rollouts]\n",
    "test_rewards = [r[2] for r in test_rollouts]\n",
    "print(\"Average Train Reward: \" + str(np.average(train_rewards)) + \" Average Test Reward: \" + str(np.average(test_rewards)))\n",
    "for k, epoch in enumerate(tqdm.tqdm(range(epoch + 1))):\n",
    "    state_action_b, target_q_values, groups = nfq_agent.generate_pattern_set(train_rollouts)\n",
    "\n",
    "    if not nfq_net.freeze_shared:\n",
    "        loss = nfq_agent.train((state_action_b, target_q_values, groups))\n",
    "\n",
    "    eval_episode_length_fg, eval_success_fg, eval_episode_cost_fg = 0, 0, 0\n",
    "    if nfq_net.freeze_shared:\n",
    "        eval_fg += 1\n",
    "        if eval_fg > 50:\n",
    "            loss = nfq_agent.train((state_action_b, target_q_values, groups))\n",
    "\n",
    "    (eval_episode_length_bg, eval_success_bg, eval_episode_cost_bg) = nfq_agent.evaluate_car(eval_env_bg, render=False)\n",
    "    #(eval_episode_length_fg,eval_success_fg, eval_episode_cost_fg) = nfq_agent.evaluate_car(eval_env_fg, render=False)\n",
    "\n",
    "    bg_success_queue = bg_success_queue[1:]\n",
    "    bg_success_queue.append(1 if eval_success_bg else 0)\n",
    "\n",
    "    #fg_success_queue = fg_success_queue[1:]\n",
    "    #fg_success_queue.append(1 if eval_success_fg else 0)\n",
    "\n",
    "    if sum(bg_success_queue) == 3 and not nfq_net.freeze_shared == True:\n",
    "        if epochs_fg == 0:\n",
    "            epochs_fg = epoch\n",
    "        printed_bg = True\n",
    "        nfq_net.freeze_shared = True\n",
    "        if verbose:\n",
    "            print(\"FREEZING SHARED\")\n",
    "        if is_contrastive:\n",
    "            for param in nfq_net.layers_shared.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in nfq_net.layers_last_shared.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in nfq_net.layers_fg.parameters():\n",
    "                param.requires_grad = True\n",
    "            for param in nfq_net.layers_last_fg.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            for param in nfq_net.layers_fg.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in nfq_net.layers_last_fg.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            optimizer = optim.Adam(\n",
    "                itertools.chain(\n",
    "                    nfq_net.layers_fg.parameters(),\n",
    "                    nfq_net.layers_last_fg.parameters(),\n",
    "                ),\n",
    "                lr=1e-1,\n",
    "            )\n",
    "            nfq_agent._optimizer = optimizer\n",
    "\n",
    "    if sum(fg_success_queue) == 3:\n",
    "        printed_fg = True\n",
    "        break\n",
    "    \n",
    "    train_rollouts, train_env_bg, train_env_fg = generate_data(bg_only=True, continuous=False, agent=nfq_agent, dataset='train')\n",
    "    test_rollouts, eval_env_bg, eval_env_fg = generate_data(bg_only=True, continuous=False, agent=nfq_agent, dataset='train')\n",
    "    train_rewards = [r[2] for r in train_rollouts]\n",
    "    test_rewards = [r[2] for r in test_rollouts]\n",
    "    print(\"Average Train Reward: \" + str(np.average(train_rewards)) + \" Average Test Reward: \" + str(np.average(test_rewards)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a new network with the result of better rollouts"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_rollouts, train_env_bg, train_env_fg = generate_data(init_experience=200, bg_only=True, continuous=False)\n",
    "test_rollouts, eval_env_bg, eval_env_fg = generate_data(init_experience=200, bg_only=True, continuous=False)\n",
    "\n",
    "is_contrastive=False\n",
    "epoch = 1000\n",
    "hint_to_goal = False\n",
    "if hint_to_goal:\n",
    "    goal_state_action_b, goal_target_q_values = train_env_bg.get_goal_pattern_set()\n",
    "    goal_state_action_b = torch.FloatTensor(goal_state_action_b)\n",
    "    goal_target_q_values = torch.FloatTensor(goal_target_q_values)\n",
    "    \n",
    "nfq_net = ContrastiveNFQNetwork(\n",
    "    state_dim=train_env_bg.state_dim, is_contrastive=is_contrastive, deep=False\n",
    ")\n",
    "optimizer = optim.Adam(nfq_net.parameters(), lr=1e-1)\n",
    "\n",
    "nfq_agent = NFQAgent(nfq_net, optimizer)\n",
    "\n",
    "bg_success_queue = [0] * 3\n",
    "fg_success_queue = [0] * 3\n",
    "epochs_fg = 0\n",
    "eval_fg = 0\n",
    "evaluations = 5\n",
    "for k, ep in enumerate(tqdm.tqdm(range(epoch + 1))):\n",
    "    state_action_b, target_q_values, groups = nfq_agent.generate_pattern_set(train_rollouts)\n",
    "    if hint_to_goal:\n",
    "        state_action_b = torch.cat([state_action_b, goal_state_action_b], dim=0)\n",
    "        target_q_values = torch.cat([target_q_values, goal_target_q_values], dim=0)\n",
    "\n",
    "    if not nfq_net.freeze_shared:\n",
    "        loss = nfq_agent.train((state_action_b, target_q_values, groups))\n",
    "\n",
    "    eval_episode_length_fg, eval_success_fg, eval_episode_cost_fg = 0, 0, 0\n",
    "    if nfq_net.freeze_shared:\n",
    "        eval_fg += 1\n",
    "        if eval_fg > 50:\n",
    "            loss = nfq_agent.train((state_action_b, target_q_values, groups))\n",
    "\n",
    "    (eval_episode_length_bg, eval_success_bg, eval_episode_cost_bg) = nfq_agent.evaluate_car(eval_env_bg, render=False)\n",
    "    bg_success_queue = bg_success_queue[1:]\n",
    "    bg_success_queue.append(1 if eval_success_bg else 0)\n",
    "\n",
    "    if sum(bg_success_queue) == 3 and not nfq_net.freeze_shared == True:\n",
    "        if epochs_fg == 0:\n",
    "            epochs_fg = epoch\n",
    "        printed_bg = True\n",
    "        nfq_net.freeze_shared = True\n",
    "        print(\"FREEZING SHARED\")\n",
    "        if is_contrastive:\n",
    "            for param in nfq_net.layers_shared.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in nfq_net.layers_last_shared.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in nfq_net.layers_fg.parameters():\n",
    "                param.requires_grad = True\n",
    "            for param in nfq_net.layers_last_fg.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            for param in nfq_net.layers_fg.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in nfq_net.layers_last_fg.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            optimizer = optim.Adam(\n",
    "                itertools.chain(\n",
    "                    nfq_net.layers_fg.parameters(),\n",
    "                    nfq_net.layers_last_fg.parameters(),\n",
    "                ),\n",
    "                lr=1e-1,\n",
    "            )\n",
    "            nfq_agent._optimizer = optimizer\n",
    "        break\n",
    "    \n",
    "    if ep % 100 == 0:\n",
    "        for it in range(evaluations):\n",
    "            (eval_episode_length_bg,eval_success_bg,eval_episode_cost_bg) = nfq_agent.evaluate_car(eval_env_bg, render=False)\n",
    "            print(eval_episode_length_bg, eval_success_bg, eval_episode_cost_bg)\n",
    "            train_env_bg.close()\n",
    "            eval_env_bg.close()\n",
    "for it in range(evaluations*10):\n",
    "    (eval_episode_length_bg,eval_success_bg,eval_episode_cost_bg) = nfq_agent.evaluate_car(eval_env_bg, render=False)\n",
    "    print(eval_episode_length_bg, eval_success_bg, eval_episode_cost_bg)\n",
    "    eval_env_bg.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using foreground and background samples\n",
    "* May need to reparameterize the fg specific layers. \n",
    "* If we reverse: we learn fg way easier, need to force it to learn bg first (or just generally change the training regime). Or better convergence criteria. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/3001 [00:00<20:10,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation bg: [100, 100, 0, 100, 0] Evaluation fg: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 16/3001 [00:02<08:11,  6.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG Trained\n",
      "FG Trained\n",
      "FG Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 19/3001 [00:03<06:44,  7.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG Trained\n",
      "FG Trained\n",
      "FG Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 22/3001 [00:03<05:58,  8.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FREEZING SHARED\n",
      "FG Trained\n",
      "FG Trained\n",
      "FG Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 23/3001 [00:03<05:47,  8.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG Trained\n",
      "FG Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 26/3001 [00:04<05:55,  8.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG Trained\n",
      "FG Trained\n",
      "FG Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 28/3001 [00:04<05:30,  9.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG Trained\n",
      "FG Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 31/3001 [00:04<05:37,  8.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG Trained\n",
      "FG Trained\n",
      "FG Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 34/3001 [00:04<06:15,  7.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG Trained\n",
      "FG Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 36/3001 [00:05<06:01,  8.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG Trained\n",
      "FG Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 39/3001 [00:05<06:23,  7.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG Trained\n",
      "FG Trained\n",
      "FG Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 42/3001 [00:05<05:42,  8.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG Trained\n",
      "FG Trained\n",
      "FG Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▏         | 44/3001 [00:06<05:43,  8.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG Trained\n",
      "FG Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 47/3001 [00:06<05:31,  8.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG Trained\n",
      "FG Trained\n",
      "FG Trained\n",
      "FG Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 49/3001 [00:06<05:48,  8.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG Trained\n",
      "FG Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 54/3001 [00:07<05:26,  9.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG Trained\n",
      "FG Trained\n",
      "FG Trained\n",
      "FG Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 56/3001 [00:07<05:31,  8.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG Trained\n",
      "FG Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 59/3001 [00:07<05:41,  8.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG Trained\n",
      "FG Trained\n",
      "FG Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 61/3001 [00:08<05:37,  8.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG Trained\n",
      "FG Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 63/3001 [00:08<06:02,  8.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG Trained\n",
      "FG Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 65/3001 [00:08<05:42,  8.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG Trained\n",
      "FG Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 67/3001 [00:08<05:28,  8.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG Trained\n",
      "FG Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 69/3001 [00:08<05:58,  8.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG Trained\n",
      "FG Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 71/3001 [00:09<05:56,  8.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG Trained\n",
      "FG Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 74/3001 [00:09<06:09,  7.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG Trained\n",
      "FG Trained\n",
      "FG Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 76/3001 [00:09<05:55,  8.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG Trained\n",
      "FG Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 77/3001 [00:10<06:45,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG Trained\n",
      "FG Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 302/3001 [00:49<12:49,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation bg: [0, 0, 0, 100, 100] Evaluation fg: [0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 602/3001 [01:43<12:17,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation bg: [0, 0, 0, 0, 0] Evaluation fg: [0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 816/3001 [02:22<05:10,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG Trained\n",
      "FG Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 818/3001 [02:22<04:56,  7.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG Trained\n",
      "FG Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 819/3001 [02:22<04:48,  7.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG Trained\n",
      "FG Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 826/3001 [02:23<05:03,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 835/3001 [02:24<04:34,  7.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FG Trained\n",
      "FG Trained\n",
      "FG Trained\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 902/3001 [02:37<10:06,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation bg: [0, 0, 100, 0, 100] Evaluation fg: [0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 1202/3001 [03:32<08:37,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation bg: [0, 0, 0, 100, 100] Evaluation fg: [0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 1313/3001 [03:52<05:12,  5.39it/s]"
     ]
    }
   ],
   "source": [
    "train_rollouts, train_env_bg, train_env_fg = generate_data(init_experience=200, bg_only=False, continuous=False)\n",
    "test_rollouts, eval_env_bg, eval_env_fg = generate_data(init_experience=200, bg_only=False, continuous=False)\n",
    "\n",
    "is_contrastive=True\n",
    "epoch = 3000\n",
    "hint_to_goal = True\n",
    "if hint_to_goal:\n",
    "    goal_state_action_b_bg, goal_target_q_values_bg, group_bg = train_env_bg.get_goal_pattern_set(group=0)\n",
    "    goal_state_action_b_fg, goal_target_q_values_fg, group_fg = train_env_fg.get_goal_pattern_set(group=1)\n",
    "    \n",
    "    goal_state_action_b_bg = torch.FloatTensor(goal_state_action_b_bg)\n",
    "    goal_target_q_values_bg = torch.FloatTensor(goal_target_q_values_bg)\n",
    "    goal_state_action_b_fg = torch.FloatTensor(goal_state_action_b_fg)\n",
    "    goal_target_q_values_fg = torch.FloatTensor(goal_target_q_values_fg)\n",
    "    \n",
    "nfq_net = ContrastiveNFQNetwork(state_dim=train_env_bg.state_dim, is_contrastive=is_contrastive, deep=False)\n",
    "optimizer = optim.Adam(nfq_net.parameters(), lr=1e-1)\n",
    "\n",
    "nfq_agent = NFQAgent(nfq_net, optimizer)\n",
    "\n",
    "bg_success_queue = [0] * 3\n",
    "fg_success_queue = [0] * 3\n",
    "eval_fg = 0\n",
    "evaluations = 5\n",
    "for k, ep in enumerate(tqdm.tqdm(range(epoch + 1))):\n",
    "    state_action_b, target_q_values, groups = nfq_agent.generate_pattern_set(train_rollouts)\n",
    "    if hint_to_goal:\n",
    "        goal_state_action_b = torch.cat([goal_state_action_b_bg, goal_state_action_b_fg], dim=0)\n",
    "        goal_target_q_values = torch.cat([goal_target_q_values_bg, goal_target_q_values_fg], dim=0)\n",
    "        state_action_b = torch.cat([state_action_b, goal_state_action_b], dim=0)\n",
    "        target_q_values = torch.cat([target_q_values, goal_target_q_values], dim=0)\n",
    "        goal_groups = torch.cat([group_bg, group_fg], dim=0)\n",
    "        groups = torch.cat([groups, goal_groups], dim=0)\n",
    "\n",
    "    if not nfq_net.freeze_shared:\n",
    "        loss = nfq_agent.train((state_action_b, target_q_values, groups))\n",
    "\n",
    "    eval_episode_length_fg, eval_success_fg, eval_episode_cost_fg = 0, 0, 0\n",
    "    if nfq_net.freeze_shared:\n",
    "        eval_fg += 1\n",
    "        if eval_fg > 50:\n",
    "            loss = nfq_agent.train((state_action_b, target_q_values, groups))\n",
    "\n",
    "    (eval_episode_length_bg, eval_success_bg, eval_episode_cost_bg) = nfq_agent.evaluate_car(eval_env_bg, render=False)\n",
    "    bg_success_queue = bg_success_queue[1:]\n",
    "    bg_success_queue.append(1 if eval_success_bg else 0)\n",
    "    \n",
    "    (eval_episode_length_fg, eval_success_fg, eval_episode_cost_fg) = nfq_agent.evaluate_car(eval_env_fg, render=False)\n",
    "    fg_success_queue = fg_success_queue[1:]\n",
    "    fg_success_queue.append(1 if eval_success_fg else 0)\n",
    "\n",
    "    if sum(bg_success_queue) == 3 and not nfq_net.freeze_shared == True:\n",
    "        nfq_net.freeze_shared = True\n",
    "        print(\"FREEZING SHARED\")\n",
    "        if is_contrastive:\n",
    "            for param in nfq_net.layers_shared.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in nfq_net.layers_last_shared.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in nfq_net.layers_fg.parameters():\n",
    "                param.requires_grad = True\n",
    "            for param in nfq_net.layers_last_fg.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            for param in nfq_net.layers_fg.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in nfq_net.layers_last_fg.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        optimizer = optim.Adam(\n",
    "            itertools.chain(\n",
    "                nfq_net.layers_fg.parameters(),\n",
    "                nfq_net.layers_last_fg.parameters(),\n",
    "            ),\n",
    "            lr=1e-1,\n",
    "        )\n",
    "        nfq_agent._optimizer = optimizer\n",
    "    if sum(fg_success_queue) == 3:\n",
    "        print(\"FG Trained\")\n",
    "        \n",
    "    if ep % 300 == 0:\n",
    "        perf_bg = []\n",
    "        perf_fg = []\n",
    "        for it in range(evaluations):\n",
    "            (eval_episode_length_bg,eval_success_bg,eval_episode_cost_bg) = nfq_agent.evaluate_car(eval_env_bg, render=False)\n",
    "            (eval_episode_length_fg,eval_success_fg,eval_episode_cost_fg) = nfq_agent.evaluate_car(eval_env_fg, render=False)\n",
    "            perf_bg.append(eval_episode_cost_bg)\n",
    "            perf_fg.append(eval_episode_cost_fg)\n",
    "            train_env_bg.close()\n",
    "            train_env_fg.close()\n",
    "            eval_env_bg.close()\n",
    "            eval_env_fg.close()\n",
    "        print(\"Evaluation bg: \" + str(perf_bg) + \" Evaluation fg: \" + str(perf_fg))\n",
    "perf_bg = []\n",
    "perf_fg = []\n",
    "for it in range(evaluations*10):\n",
    "    (eval_episode_length_bg,eval_success_bg,eval_episode_cost_bg) = nfq_agent.evaluate_car(eval_env_bg, render=False)\n",
    "    (eval_episode_length_fg,eval_success_fg,eval_episode_cost_fg) = nfq_agent.evaluate_car(eval_env_fg, render=False)\n",
    "    perf_bg.append(eval_episode_cost_bg)\n",
    "    perf_fg.append(eval_episode_cost_fg)\n",
    "    eval_env_bg.close()\n",
    "    eval_env_fg.close()\n",
    "print(\"Evaluation bg: \" + str(sum(perf_bg)/len(perf_bg)) + \" Evaluation fg: \" + str(sum(perf_fg)/len(perf_fg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research [~/.conda/envs/research/]",
   "language": "python",
   "name": "conda_research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
