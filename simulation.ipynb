{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle, os, csv, math, time, joblib\n",
    "from joblib import Parallel, delayed\n",
    "import datetime as dt\n",
    "from datetime import date, datetime, timedelta\n",
    "from collections import Counter\n",
    "import copy as cp\n",
    "import tqdm\n",
    "from sklearn.ensemble import ExtraTreesRegressor, ExtraTreesClassifier\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import log_loss, f1_score, precision_score, recall_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import collections \n",
    "#import shap\n",
    "import seaborn as sns\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation\n",
    "* Simulate RL data from two different distributions, generate transition tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate transition matrices, separate distributions for each one\n",
    "shape, scale = 2., 1. \n",
    "transition_foreground = np.random.gamma(shape, scale, (12, 10))\n",
    "\n",
    "mu, sigma = 0, 0.1 # mean and standard deviation\n",
    "transition_background = np.random.normal(mu, sigma, (12, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate reward function\n",
    "mu, sigma = 0, 2\n",
    "reward_function = np.random.normal(mu, sigma, (12, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "exploit = 0.8\n",
    "explore = 1-exploit\n",
    "num_samples = 10000\n",
    "actions = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "mu, sigma = 0, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_tuples = []\n",
    "for i in range(num_samples):\n",
    "    # All initial states are generated from random normal\n",
    "    s = np.random.normal(mu, sigma, (10, 1))\n",
    "    \n",
    "    flip = random.uniform(0, 1)\n",
    "    # Exploit\n",
    "    if flip < exploit:\n",
    "        # Decide which transition matrix\n",
    "        flip = np.random.choice(1)\n",
    "        \n",
    "        all_rewards = []\n",
    "        for j, a in enumerate(actions):\n",
    "            a = np.asarray(a)\n",
    "            a = np.reshape(a, (2, 1))\n",
    "            s_a = np.concatenate((s, a))\n",
    "            reward = np.dot(reward_function.T, s_a)\n",
    "            all_rewards.append(reward)\n",
    "        \n",
    "        noise = np.random.normal(0, 0.01, 1)\n",
    "        all_rewards = np.asarray(all_rewards)\n",
    "        a = actions[np.argmax(all_rewards)]\n",
    "        reward = np.max(all_rewards) + noise\n",
    "        \n",
    "        if flip == 0:\n",
    "            ns = np.dot(s_a.T, transition_foreground) \n",
    "        else:\n",
    "            ns = np.dot(s_a.T, transition_background) \n",
    "        ns = np.add(ns , np.random.normal(0, 0.01, (1, 10))) # Add noise\n",
    "    # Explore\n",
    "    else:\n",
    "        a = np.asarray(actions[np.random.choice(3)])\n",
    "        a = np.reshape(a, (2, 1))\n",
    "        s_a = np.concatenate((s, a)) # concatenate the state and action\n",
    "        \n",
    "        # Decide which transition matrix\n",
    "        flip = np.random.choice(1)\n",
    "        if flip == 0:\n",
    "            ns = np.dot(s_a.T, transition_foreground)\n",
    "        else:\n",
    "            ns = np.dot(s_a.T, transition_background)\n",
    "        reward = np.dot(reward_function.T, s_a) + np.random.normal(0, 0.01, 1)\n",
    "        ns = np.add(ns , np.random.normal(0, 0.01, (1, 10))) # Add noise\n",
    "    \n",
    "    # Transition tuple includes state, action, next state, reward, indication of foreground/background\n",
    "    # 1 if foreground 0 if background\n",
    "    transition_tuples.append((s, list(a), ns, reward, 1 - flip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(0.8*len(transition_tuples))\n",
    "train_tuples = transition_tuples[:split]\n",
    "test_tuples = transition_tuples[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = {}\n",
    "test = {}\n",
    "elts = ['s', 'a', 'ns', 'r', 'ds']\n",
    "for elt in elts:\n",
    "    train[elt] = []\n",
    "    test[elt] = []\n",
    "\n",
    "for tup in train_tuples:\n",
    "    train['s'].append(tup[0])\n",
    "    train['a'].append(tup[1])\n",
    "    train['ns'].append(tup[2])\n",
    "    train['r'].append(tup[3])\n",
    "    train['ds'].append(tup[4])\n",
    "\n",
    "for tup in test_tuples:\n",
    "    test['s'].append(tup[0])\n",
    "    test['a'].append(tup[1])\n",
    "    test['ns'].append(tup[2])\n",
    "    test['r'].append(tup[3])\n",
    "    test['ds'].append(tup[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FQI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fit vanilla FQI algorithm, with linear regression on both data\n",
    "* Do mixed model linear contrastive case\n",
    "* What do we expect when we validate it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define action space, the potential classes of action items. \n",
    "def a2c(a):\n",
    "    actions = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "    classes = []\n",
    "    for action in a:\n",
    "        c = np.where(actions==action)\n",
    "        classes.append(c)\n",
    "    \n",
    "    return classes\n",
    "        \n",
    "# Mapping states to actions?        \n",
    "def c2a(c):\n",
    "    d = {0: [0, 0], 1: [0, 1], 2: [1, 0], 3: [1, 1]}\n",
    "    return np.array([d[k] for k in c])\n",
    "\n",
    "def random_weights(size=5):\n",
    "    \n",
    "    #w = 2*np.random.uniform(size=size) - 1\n",
    "    w = norm(np.random.uniform(size=size))\n",
    "    #w / np.sum(np.abs(w))\n",
    "    \n",
    "    return w\n",
    "\n",
    "def norm(vec):\n",
    "    return vec/np.sum(np.abs(vec))\n",
    "\n",
    "def learnBehaviour(training_set, test_set):  \n",
    "    floc = \"simulated_fqi/behavior.pkl\"\n",
    "    behaviour_pi = LGBMClassifier(n_estimators=2000, class_weight='balanced', num_leaves=100)\n",
    "    behaviour_pi.fit(np.vstack((training_set['s'], test_set['s'])),\n",
    "                         a2c(np.vstack((training_set['a'], test_set['a']))))\n",
    "    pickle.dump(behaviour_pi, open(floc, 'wb'))    \n",
    "    \n",
    "    return behaviour_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FQIagent():\n",
    "    def __init__(self, train_tuples, test_tuples, iters=100, gamma=0.99, batch_size=100, prioritize=True, estimator='gbm',\n",
    "                 weights=np.array([1, 1, 1, 1, 1])/5., maxT=36):\n",
    "        \n",
    "        self.iters = iters\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.prioritize_a = prioritize\n",
    "        self.training_set = train_tuples\n",
    "        self.test_set = test_tuples\n",
    "        self.visits = {'train': len(train_tuples), 'test': len(test_tuples)}\n",
    "        self.NV = {'train': len(train_tuples), 'test': len(test_tuples)}\n",
    "        self.n_samples = len(test_tuples)\n",
    "        self.actions, self.unique_actions, self.action_counts, self.n_actions = self.sub_actions()\n",
    "        self.state_feats = [str(x) for x in range(10)]\n",
    "        self.n_features = len(self.state_feats)\n",
    "        self.reward_weights = weights\n",
    "        self.maxT = maxT\n",
    "        self.piB = learnBehaviour(self.training_set, self.test_set)\n",
    "        \n",
    "        if estimator == 'tree':\n",
    "            self.q_est = {}\n",
    "            for k in self.piB.keys():\n",
    "                self.q_est[k] = ExtraTreesRegressor(n_estimators=50, max_depth=None, min_samples_leaf=10, min_samples_split=2,\n",
    "                                             random_state=0)\n",
    "        elif estimator == 'gbm':\n",
    "            self.q_est = {}\n",
    "            for k in self.piB.keys():\n",
    "                self.q_est[k] = LGBMRegressor(n_estimators=50, silent=True)\n",
    "        elif estimator == 'nn':\n",
    "            self.q_est = None\n",
    "            \n",
    "        self.piE = {}\n",
    "        for k in self.piB.keys():\n",
    "            self.piE[k] = LGBMClassifier(n_estimators=50, silent=True)\n",
    "        \n",
    "        self.eval_est = LGBMRegressor(n_estimators=50, silent=True)\n",
    "\n",
    "    def sub_actions(self):\n",
    "        \n",
    "        a = self.training_set['a']\n",
    "\n",
    "        unique_actions = 0\n",
    "        action_counts = 0\n",
    "        n_actions = 0\n",
    "        \n",
    "        unique_actions, action_counts = np.unique(a, axis=0, return_counts=True)\n",
    "        n_actions = len(unique_actions)\n",
    "                \n",
    "        return actions, unique_actions, action_counts, n_actions\n",
    "        \n",
    "    def sample_probs(self):\n",
    "        \n",
    "        if self.prioritize_a:\n",
    "            sample_probs = 1.*np.ones(self.n_samples)/self.n_samples\n",
    "            for i, a in enumerate(self.unique_actions[self.ak]):\n",
    "                sample_probs[np.where((self.actions[self.ak]==a).all(axis=1))[0]] = 1./self.action_counts[self.ak][i]\n",
    "\n",
    "            sample_probs = sample_probs/np.sum(sample_probs)\n",
    "        else:\n",
    "            sample_probs = None\n",
    "            \n",
    "        return sample_probs\n",
    "    \n",
    "    def sampleTuples(self):\n",
    "    \n",
    "        # Get batch of (prioritized) samples\n",
    "        v = self.training_set['vnum']\n",
    "        choose_from = list(set(range(self.n_samples)) - set(np.where(np.roll(v,1)!=v)[0] - 1))[:-1]\n",
    "        self.batch_size = len(choose_from)\n",
    "        probs = self.sample_probs()\n",
    "        if probs is not None:\n",
    "            probs = probs[choose_from]/np.sum(probs[choose_from])\n",
    "        ids = np.random.choice(choose_from, self.batch_size, replace=False, p=probs)\n",
    "        batch = {}\n",
    "        for k in self.training_set.keys():\n",
    "            batch[k] = self.training_set[k][ids]\n",
    "        batch['a'] = self.actions[self.ak][ids]\n",
    "        batch['r'] = np.dot(batch['phi']*[1, 1, 10, 10, 100], self.reward_weights)\n",
    "        #print(batch['phi'], batch['r'], batch['a'])\n",
    "        batch['s_ids'] = ids\n",
    "        batch['ns_ids'] = ids + 1\n",
    "    \n",
    "        return batch\n",
    "    \n",
    "    def fitQ(self, batch, Q):\n",
    "        \n",
    "        # input = [state action]\n",
    "        x =  np.hstack((batch['s'], batch['a']))\n",
    "        \n",
    "        # target = r + gamma * max_a(Q(s', a))      == r for first iteration\n",
    "        y = batch['r'] + (self.gamma * np.max(Q[batch['ns_ids'], :], axis=1))\n",
    "        \n",
    "        self.q_est[self.ak].fit(x, y)   \n",
    "    \n",
    "    def updateQtable(self, Qtable, batch):\n",
    "        \n",
    "        for i, a in enumerate(self.unique_actions[self.ak]):\n",
    "            #print(a, i)\n",
    "            Qtable[batch['s_ids'], i] = self.q_est[self.ak].predict(np.hstack((batch['ns'], np.tile(a, (self.batch_size,1)))))\n",
    "        return Qtable\n",
    "    \n",
    "    def runFQI(self, repeats=1):\n",
    "        \n",
    "        print('Learning policy:', self.ak)\n",
    "        meanQtable = np.zeros((self.n_samples, self.n_actions[self.ak]))\n",
    "        \n",
    "        for r in range(repeats):\n",
    "            print('Run', r, ':')\n",
    "            print('Initialize: get batch, set initial Q')\n",
    "            Qtable = np.zeros((self.n_samples, self.n_actions[self.ak]))\n",
    "            Qdist = []\n",
    "\n",
    "            #print('Run FQI')\n",
    "            for iteration in range(self.iters):\n",
    "\n",
    "                # copy q-table\n",
    "                Qold = cp.deepcopy(Qtable)\n",
    "\n",
    "                # sample batch  \n",
    "                batch = self.sampleTuples() \n",
    "\n",
    "                # learn q_est with samples, targets from batch\n",
    "                self.fitQ(batch, Qtable)\n",
    "\n",
    "                # update Q table for all s given new estimator\n",
    "                self.updateQtable(Qtable, batch)\n",
    "\n",
    "                # check divergence from last estimate\n",
    "                Qdist.append(mean_absolute_error(Qold, Qtable))\n",
    "         \n",
    "            #plt.plot(Qdist)\n",
    "            meanQtable += Qtable\n",
    "        \n",
    "        meanQtable = meanQtable / repeats\n",
    "        print('Learn policy')\n",
    "        self.getPi(meanQtable)\n",
    "        return Qdist\n",
    "                    \n",
    "    \n",
    "    def getPi(self, Qtable):\n",
    "        optA = np.argmax(Qtable, axis=1)\n",
    "        #print(\"Fitting to training set\")\n",
    "        #print(\"Optimal actions: \", optA)\n",
    "        self.piE[self.ak].fit(self.training_set['s'], optA)\n",
    "        #print(\"Done Fitting\")\n",
    "        \n",
    "        \n",
    "    def testPi(self, vnum, behavior):\n",
    "\n",
    "        if vnum in self.test_set['vnum']: data = self.test_set\n",
    "        elif vnum in self.training_set['vnum']: data = self.training_set\n",
    "\n",
    "        inds = np.where(data['vnum']==vnum)[0]\n",
    "        #evalA = self.piE['cat'].predict(data['s'][inds])\n",
    "        #behavA = a2c(data['a'][inds], 'cat')\n",
    "        \n",
    "        # actions based on policy we learn\n",
    "        evalA = self.piE[self.ak].predict(data['s'][inds])\n",
    "        # actual historical actions\n",
    "        behavA = a2c(data['a'][inds], self.ak)\n",
    "        # predicted actions based on historical actions model\n",
    "        behavB = behavior[self.ak].predict(data['s'][inds])\n",
    "\n",
    "        #plt.title(vnum)\n",
    "\n",
    "        return inds, evalA, behavA, behavB\n",
    "               \n",
    "    def fitQeval(self, batch, Q):\n",
    "        \n",
    "        # input = [state action]\n",
    "        x =  np.hstack((batch['s'], batch['a']))\n",
    "                \n",
    "        # target = r + gamma * (Q(s', pi(a)))   \n",
    "        expected_a = self.piE['cat'].predict(batch['ns'])\n",
    "        y = batch['r'] + (self.gamma * (Q[batch['ns_ids'], expected_a]))\n",
    "        \n",
    "        self.eval_est.fit(x, y)\n",
    "\n",
    "    def updateQEtable(self, Qtable, batch):\n",
    "        \n",
    "        for i, a in enumerate(self.unique_actions[self.ak]):\n",
    "            #print(a, i)\n",
    "            Qtable[batch['s_ids'], i] = self.eval_est.predict(np.hstack((batch['ns'], np.tile(a, (self.batch_size,1)))))\n",
    "        return Qtable\n",
    "    \n",
    "    def runFQE(self):\n",
    "        \n",
    "        print('Initialize: get batch, set initial Q')\n",
    "        QEtable = np.random.rand(self.n_samples, self.n_actions[self.ak]) - 1\n",
    "        QEdist = []\n",
    "        \n",
    "        print('Run FQE')\n",
    "        for iteration in range(self.iters):\n",
    "            \n",
    "            # copy q-table\n",
    "            QEold = cp.deepcopy(QEtable)\n",
    "              \n",
    "            # sample batch  \n",
    "            batch = self.sampleTuples() \n",
    "             \n",
    "            # learn q_est with samples, targets from batch\n",
    "            self.fitQeval(batch, QEtable)\n",
    "              \n",
    "            # update Q table for all s given new estimator\n",
    "            self.updateQEtable(QEtable, batch)\n",
    "              \n",
    "            # check divergence from last estimate\n",
    "            QEdist.append(mean_absolute_error(QEold, QEtable))\n",
    "                     \n",
    "        plt.plot(QEdist)\n",
    "        \n",
    "        Vvals = np.max(QEtable, axis=1)\n",
    "        Vest = LGBMRegressor(n_estimators=100, silent=False)\n",
    "        Vest.fit(self.training_set['s'], Vvals)\n",
    "        \n",
    "        estimated_v = Vest.predict(self.test_set['s'])\n",
    "        \n",
    "        return estimated_v\n",
    "\n",
    "    def getEpisode(self, v, vset='test'):\n",
    "        \n",
    "        if vset=='test': dataset = self.test_set\n",
    "        else: dataset = self.training_set\n",
    "        ep = {}\n",
    "        inds = np.where(dataset['vnum'] == v)[0]\n",
    "        for k in dataset.keys():\n",
    "            ep[k] = dataset[k][inds]\n",
    "            \n",
    "        states = ep['s']\n",
    "        actions = a2c(ep['a'], ak='cat')\n",
    "        phis = ep['phi']\n",
    "        rewards = np.dot(ep['phi'], self.reward_weights)\n",
    "        \n",
    "        return states, actions, phis, rewards\n",
    "\n",
    "    def getRhos(self, W=False, PD=True, vset='test'):\n",
    "\n",
    "        rhos = np.ones([self.NV[vset], self.maxT])\n",
    "        epRhos = []\n",
    "\n",
    "        for i, v in enumerate(self.visits[vset]):\n",
    "            \n",
    "            # load episode\n",
    "            states, actions, phis, rewards = self.getEpisode(v, vset)\n",
    "            epT = len(states)\n",
    "            T = min(epT, self.maxT)            \n",
    "            \n",
    "            # load action probabilities (unclear why the T isn't appropriate here)\n",
    "            #print(str(self.piB['cat'].predict_proba(states).shape))\n",
    "            #print(str(np.arange(epT)))\n",
    "            #print(str(actions))\n",
    "            #print(str(np.arange(epT), actions))\n",
    "            prob_b = self.piB['cat'].predict_proba(states)[np.arange(epT), actions][:T]\n",
    "            #print(str(prob_b))\n",
    "            if self.piE['cat'].predict_proba(states).shape[1] < 4:\n",
    "                #print('Warning: < 4 classes')\n",
    "                prob_e = np.zeros([T, 4])\n",
    "                cols = np.unique(self.piE['cat'].predict(states))\n",
    "                probs = self.piE['cat'].predict_proba(states)\n",
    "                for i, a in enumerate(cols):\n",
    "                    prob_e[:, a] = probs[:, i]\n",
    "                prob_e = prob_e[np.arange(epT), actions][:T]\n",
    "            else:\n",
    "                prob_e = self.piE['cat'].predict_proba(states)[np.arange(epT), actions][:T]\n",
    "            \n",
    "            # calculate importance weights\n",
    "            if PD:\n",
    "                # per-step cumulative weights\n",
    "                invprop = np.cumprod(prob_e/prob_b, axis=0)\n",
    "                # clip importance weights\n",
    "                invprop[invprop<1e-3] = 1e-3\n",
    "                invprop[invprop>1e3] = 1e3\n",
    "                \n",
    "                rhos[i, :len(invprop)] = list(invprop)\n",
    "                rhos[i, len(invprop):] = np.ones(self.maxT-len(invprop)) * rhos[i,len(invprop)-1]\n",
    "                epRhos.append({'s': states, 'a': actions, 'phi': phis,'r': rewards})  \n",
    "        norm = self.NV[vset]\n",
    "        if W: norm = np.sum(rhos, axis=0)    \n",
    "        for i in range(self.NV[vset]): \n",
    "            epRhos[i]['rho'] = rhos[i,:] / norm\n",
    "            \n",
    "        return epRhos\n",
    "            \n",
    "    def perdecisionIS(self, vset='test', W=False):\n",
    "        \n",
    "        rhos = self.getRhos(W=W)\n",
    "        T = [len(rhos[i]['phi']) for i in range(len(rhos))]        \n",
    "        gamma_vec = [self.gamma**(i+1) for i in range(self.maxT)]                    \n",
    "        \n",
    "        estimated_mu = np.vstack([np.sum(rhos[i]['phi'][:T[i]] * (gamma_vec[:T[i]] * rhos[i]['rho'][:T[i]])[:,np.newaxis],\n",
    "                                         axis=0) for i in range(self.NV[vset])])\n",
    "        \n",
    "        estimated_V = np.dot(estimated_mu, np.array(self.reward_weights))\n",
    "                \n",
    "        return estimated_mu, estimated_V        \n",
    "    \n",
    "\n",
    "    def find_feature_expectations(self, rhos, behav=False, vset='train'):\n",
    "\n",
    "        gamma_vec = [self.gamma**(i+1) for i in range(self.maxT)]\n",
    "        T = [len(rhos[i]['phi']) for i in range(len(rhos))]  \n",
    "\n",
    "        if behav:\n",
    "            print('Simple averaging')\n",
    "            estimated_mu = np.mean(np.vstack([np.sum(rhos[i]['phi'] * np.array(gamma_vec[:T[i]])[:,np.newaxis], axis=0) \n",
    "                                      for i in range(self.NV[vset])]), axis=0)\n",
    "        else:\n",
    "            print('PDWIS estimate')\n",
    "            estimated_mu = np.mean(np.vstack([np.sum(rhos[i]['phi'] * (gamma_vec[:T[i]] * rhos[i]['rho'][:T[i]])[:,np.newaxis],\n",
    "                                             axis=0) for i in range(self.NV[vset])]), axis=0)\n",
    "\n",
    "        return estimated_mu\n",
    "\n",
    "\n",
    "    def irl(self, epochs=10, learning_rate=0.5, init_w=None):\n",
    "\n",
    "        # Initialize reward weights:\n",
    "        w_vecs = []\n",
    "        if init_w is None:\n",
    "            #w = random_weights(len(self.reward_weights))\n",
    "            w = np.ones(len(self.reward_weights))/float(len(self.reward_weights))\n",
    "        else:\n",
    "            w = init_w\n",
    "        feature_matrix = self.training_set['phi']\n",
    "        muB = None\n",
    "\n",
    "        for i in range(epochs):\n",
    "\n",
    "            print('Epoch', i, '- Train pi with current w=', w)\n",
    "            #r = np.dot(feature_matrix, w)\n",
    "            self.reward_weights = w\n",
    "            w_vecs.append(w)\n",
    "            #try:\n",
    "            self.runFQI()\n",
    "            print('piE:', np.unique(self.piE['cat'].predict(self.training_set['s']), return_counts=True))\n",
    "\n",
    "            print('Evaluate feature expectations for pi')\n",
    "            epRhos = self.getRhos(vset='train')\n",
    "            mu = self.find_feature_expectations(epRhos, behav=False, vset='train')\n",
    "            print(mu)\n",
    "\n",
    "            print('Initialize behaviour mu:')\n",
    "            if muB is None:\n",
    "                muB = self.find_feature_expectations(epRhos, behav=True, vset='train')\n",
    "            print(muB)\n",
    "\n",
    "            print('Gradient update for new w')\n",
    "            grad = norm(muB) - norm(mu)\n",
    "#             except:\n",
    "#                 print('Error - skip update')\n",
    "#                 grad = 0\n",
    "            w += learning_rate*(0.95**i) * grad\n",
    "            w = w/np.sum(np.abs(w))\n",
    "            \n",
    "\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'unknown'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-898f5c4643e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFQIagent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tuples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_tuples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mQ_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunFQI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"Vanilla FQI\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iteration\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Q Estimate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-59a1a70a97f3>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train_tuples, test_tuples, iters, gamma, batch_size, prioritize, estimator, weights, maxT)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpiB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearnBehaviour\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tree'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-9964b1aa9334>\u001b[0m in \u001b[0;36mlearnBehaviour\u001b[0;34m(training_set, test_set)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mbehaviour_pi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLGBMClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_leaves\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     behaviour_pi.fit(np.vstack((training_set['s'], test_set['s'])),\n\u001b[0;32m---> 31\u001b[0;31m                          a2c(np.vstack((training_set['a'], test_set['a']))))\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbehaviour_pi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.5/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[0;34m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0m_LGBMAssertAllFinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m         \u001b[0m_LGBMCheckClassificationTargets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_le\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LGBMLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0m_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gym/lib/python3.5/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    169\u001b[0m     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\n\u001b[1;32m    170\u001b[0m                       'multilabel-indicator', 'multilabel-sequences']:\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: 'unknown'"
     ]
    }
   ],
   "source": [
    "agent = FQIagent(train_tuples=train, test_tuples=test, weights=[1, 1, 1, 1, 1])\n",
    "Q_dist = agent.runFQI(repeats=1)\n",
    "plt.plot(Q_dist, label= \"Vanilla FQI\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Q Estimate\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
