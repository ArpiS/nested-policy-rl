{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle, os, csv, math, time, joblib\n",
    "from joblib import Parallel, delayed\n",
    "import datetime as dt\n",
    "from datetime import date, datetime, timedelta\n",
    "from collections import Counter\n",
    "import copy as cp\n",
    "import tqdm\n",
    "from sklearn.ensemble import ExtraTreesRegressor, ExtraTreesClassifier\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import log_loss, f1_score, precision_score, recall_score, accuracy_score\n",
    "#import matplotlib.pyplot as plt\n",
    "#import matplotlib.ticker as ticker\n",
    "import collections \n",
    "#import shap\n",
    "import seaborn as sns\n",
    "import random\n",
    "from sklearn.linear_model import LinearRegression\n",
    "np.seterr(all=\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation\n",
    "* Simulate RL data from two different distributions, generate transition tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Generate transition matrices, separate distributions for each one\n",
    "# We have to ensure that these transitions keep the next state calculations within some reasonable range\n",
    "# Make sure that states aren't exploding\n",
    "shape, scale = 2, 10\n",
    "transition_foreground = np.random.gamma(shape, scale, (12, 10))\n",
    "\n",
    "mu, sigma = 0, 4 # mean and standard deviation\n",
    "transition_background = np.random.normal(mu, sigma, (12, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Generate reward function\n",
    "mu, sigma = 0, 5\n",
    "reward_function = np.random.normal(mu, sigma, (12, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Params\n",
    "exploit = 0.6\n",
    "explore = 1-exploit\n",
    "num_samples = 100\n",
    "num_patients = 100\n",
    "#actions = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "actions = [[0, 0], [0, 2], [3, 1], [4, 4]]\n",
    "mu, sigma = 0, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 72.95it/s]\n"
     ]
    }
   ],
   "source": [
    "transition_tuples = []\n",
    "for k, pat in enumerate(tqdm.tqdm(range(num_patients))):\n",
    "    \n",
    "    flip = np.random.choice(2)\n",
    "    if flip == 0:\n",
    "        ds = 'foreground'\n",
    "    else:\n",
    "        ds = 'background'\n",
    "    # Generate a random initial state\n",
    "    s = np.random.normal(mu, sigma, (10, 1))\n",
    "    \n",
    "    # Generate all of the tuples for this patient\n",
    "    for i in range(num_samples):\n",
    "        flip = random.uniform(0, 1)\n",
    "        # Exploit\n",
    "        if flip < exploit:            \n",
    "            all_rewards = []\n",
    "            for j, a in enumerate(actions):\n",
    "                a = np.asarray(a)\n",
    "                a = np.reshape(a, (2, 1))\n",
    "                s_a = np.concatenate((s, a))\n",
    "                reward = np.dot(reward_function.T, s_a)\n",
    "                all_rewards.append(reward)\n",
    "\n",
    "            noise = np.random.normal(0, 0.05, 1)\n",
    "            all_rewards = np.asarray(all_rewards)\n",
    "            a = actions[np.argmax(all_rewards)]\n",
    "            reward = np.max(all_rewards) + noise\n",
    "            \n",
    "            if ds == 'foreground':\n",
    "                t_m = transition_foreground\n",
    "            else:\n",
    "                t_m = transition_background\n",
    "            ns = np.matmul(s_a.T, t_m) / np.linalg.norm(np.matmul(s_a.T, t_m), ord=2)\n",
    "            ns = np.add(ns, np.random.normal(0, 0.5, (1, 10))) # Add noise\n",
    "            \n",
    "        \n",
    "        # Explore\n",
    "        else:\n",
    "            a = np.asarray(actions[np.random.choice(3)])\n",
    "            a = np.reshape(a, (2, 1))\n",
    "            s_a = np.concatenate((s, a)) # concatenate the state and action\n",
    "\n",
    "            if ds == 'foreground':\n",
    "                t_m = transition_foreground\n",
    "            else:\n",
    "                t_m = transition_background\n",
    "            ns = np.matmul(s_a.T, t_m) / np.linalg.norm(np.matmul(s_a.T, t_m), ord=2)\n",
    "            ns = np.add(ns, np.random.normal(0, 0.5, (1, 10))) # Add noise\n",
    "            \n",
    "            reward = np.dot(reward_function.T, s_a) + np.random.normal(0, 0.5, 1)\n",
    "\n",
    "        # Transition tuple includes state, action, next state, reward, ds\n",
    "        transition_tuples.append((list(s.flatten()), list(a), list(ns.flatten()), list(reward.flatten())[0], ds, i))\n",
    "        s = ns.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(0.8*len(transition_tuples))\n",
    "train_tuples = transition_tuples[:split]\n",
    "test_tuples = transition_tuples[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def construct_mixedlm_ds(tuples, fname):\n",
    "    tup_dict = {}\n",
    "    elts = ['a0', 'a1', 'r', 'ds']\n",
    "    for i in range(10):\n",
    "        elts.append('s' +str(i))\n",
    "    for elt in elts:\n",
    "        tup_dict[elt] = []\n",
    "\n",
    "    for j, tup in enumerate(tqdm.tqdm(tuples)):\n",
    "        state = tup[0]\n",
    "        for i in range(len(state)):\n",
    "            tup_dict['s' + str(i)].append(state[i])\n",
    "\n",
    "        try:\n",
    "            a = tup[1]\n",
    "            a = np.concatenate(a).ravel()\n",
    "            a = list(a)\n",
    "            tup_dict['a0'].append(int(a[0]))\n",
    "            tup_dict['a1'].append(int(a[1]))\n",
    "        except:\n",
    "            a = tup[1]\n",
    "            tup_dict['a0'].append(int(a[0]))\n",
    "            tup_dict['a1'].append(int(a[1]))\n",
    "\n",
    "        tup_dict['r'].append(float(tup[3]))\n",
    "        tup_dict['ds'].append(int(tup[4] == 'foreground'))\n",
    "    \n",
    "    with open(fname, 'w') as fp:\n",
    "        json.dump(tup_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 86794.57it/s]\n"
     ]
    }
   ],
   "source": [
    "construct_mixedlm_ds(test_tuples, 'test_tuples.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [a for (a, b, c, d, e, f) in transition_tuples]\n",
    "states = np.squeeze(states)\n",
    "sns.heatmap(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = [c for (a, b, c, d, e, f) in transition_tuples]\n",
    "ns = np.squeeze(ns)\n",
    "sns.heatmap(ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = [d for (a, b, c, d, e, f) in transition_tuples]\n",
    "sns.distplot(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def construct_dicts(train_tuples, test_tuples):\n",
    "    train = {}\n",
    "    test = {}\n",
    "    elts = ['s', 'a', 'ns', 'r', 'ds', 'vnum']\n",
    "    for elt in elts:\n",
    "        train[elt] = []\n",
    "        test[elt] = []\n",
    "\n",
    "    for tup in train_tuples:\n",
    "        train['s'].append(tup[0].flatten())\n",
    "        a = tup[1]\n",
    "        try:\n",
    "            a = np.concatenate(a).ravel()\n",
    "            a = list(a)\n",
    "            train['a'].append(a)\n",
    "        except:\n",
    "            train['a'].append(a)\n",
    "        train['ns'].append(tup[2].flatten())\n",
    "        train['r'].append(tup[3])\n",
    "        train['ds'].append(tup[4])\n",
    "        train['vnum'].append(tup[5])\n",
    "\n",
    "    for tup in test_tuples:\n",
    "        test['s'].append(tup[0].flatten())\n",
    "        try:\n",
    "            a = tup[1]\n",
    "            a = np.concatenate(a).ravel()\n",
    "            a = list(a)\n",
    "            test['a'].append(a)\n",
    "        except:\n",
    "            test['a'].append(tup[1])\n",
    "        test['ns'].append(tup[2].flatten())\n",
    "        test['r'].append(tup[3])\n",
    "        test['ds'].append(tup[4])\n",
    "        test['vnum'].append(tup[5])\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FQI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FQI on both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Define action space, the potential classes of action items. \n",
    "def a2c(action):\n",
    "    #actions = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "    actions = [[0, 0], [0, 2], [3, 1], [4, 4]]\n",
    "    classes = []\n",
    "    for a in action:\n",
    "        a = list(a)\n",
    "        for c in range(len(actions)):\n",
    "            if actions[c] == a:\n",
    "                classes.append(c) \n",
    "    return classes\n",
    "\n",
    "def p2c(pred):\n",
    "    if pred <= 0.25:\n",
    "        action = [0, 0]\n",
    "    elif pred <= 0.5:\n",
    "        action = [0, 1]\n",
    "    elif pred <= 0.75:\n",
    "        action = [1, 0]\n",
    "    else:\n",
    "        action = [1, 1]\n",
    "        \n",
    "# Mapping states to actions?        \n",
    "def c2a(c):\n",
    "    d = {0: [0, 0], 1: [0, 1], 2: [1, 0], 3: [1, 1]}\n",
    "    return np.array([d[k] for k in c])\n",
    "\n",
    "def random_weights(size=5):\n",
    "    \n",
    "    #w = 2*np.random.uniform(size=size) - 1\n",
    "    w = norm(np.random.uniform(size=size))\n",
    "    #w / np.sum(np.abs(w))\n",
    "    \n",
    "    return w\n",
    "\n",
    "def norm(vec):\n",
    "    return vec/np.sum(np.abs(vec))\n",
    "\n",
    "def learnBehaviour(training_set, test_set):  \n",
    "    floc = \"simulated_fqi/behavior.pkl\"\n",
    "    #if os.path.exists(floc):\n",
    "    #    behaviour_pi = pickle.load(open(floc, 'rb'))\n",
    "    #else:\n",
    "    # Use a linear regression to predict behavior\n",
    "    behaviour_pi = LinearRegression()\n",
    "    X = np.vstack((training_set['s'], test_set['s']))\n",
    "    X = np.reshape(X, (-1, 10))\n",
    "    y = a2c(np.vstack((training_set['a'], test_set['a'])))\n",
    "    behaviour_pi.fit(X,y)\n",
    "    pickle.dump(behaviour_pi, open(floc, 'wb'))    \n",
    "    \n",
    "    return behaviour_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class FQIagent():\n",
    "    def __init__(self, train_tuples, test_tuples, iters=150, gamma=0.99, batch_size=100, prioritize=False, estimator='lin',\n",
    "                 weights=np.array([1, 1, 1, 1, 1])/5., maxT=36):\n",
    "        \n",
    "        self.iters = iters\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.prioritize_a = prioritize\n",
    "        self.training_set, self.test_set = construct_dicts(train_tuples, test_tuples)\n",
    "        self.raw_test = test_tuples\n",
    "        \n",
    "        self.visits = {'train': len(train_tuples), 'test': len(test_tuples)}\n",
    "        self.NV = {'train': len(train_tuples), 'test': len(test_tuples)}\n",
    "        self.n_samples = len(self.training_set['s'])\n",
    "        _, self.unique_actions, self.action_counts, _ = self.sub_actions()\n",
    "        self.state_feats = [str(x) for x in range(10)]\n",
    "        self.n_features = len(self.state_feats)\n",
    "        self.reward_weights = weights\n",
    "        self.maxT = maxT\n",
    "        self.piB = learnBehaviour(self.training_set, self.test_set)\n",
    "        self.n_actions = 4\n",
    "        \n",
    "        if estimator == 'tree':\n",
    "            self.q_est = ExtraTreesRegressor(n_estimators=50, max_depth=None, min_samples_leaf=10, min_samples_split=2,\n",
    "                                             random_state=0)\n",
    "        elif estimator == 'gbm':\n",
    "            self.q_est = LGBMRegressor(n_estimators=50, silent=True)\n",
    "\n",
    "        elif estimator == 'nn':\n",
    "            self.q_est = None\n",
    "        \n",
    "        elif estimator == 'lin':\n",
    "            self.q_est = LinearRegression()\n",
    "            \n",
    "        self.piE = LinearRegression()#LGBMClassifier(n_estimators=50, silent=True)\n",
    "        \n",
    "        self.eval_est = LGBMRegressor(n_estimators=50, silent=True)\n",
    "\n",
    "    def sub_actions(self):\n",
    "        \n",
    "        a = self.training_set['a']\n",
    "        a = list(a)\n",
    "        \n",
    "        unique_actions = 0\n",
    "        action_counts = 0\n",
    "        n_actions = 0\n",
    "        \n",
    "        unique_actions, action_counts = np.unique(a, axis=0, return_counts=True)\n",
    "        n_actions = len(unique_actions)\n",
    "                \n",
    "        return a, unique_actions, action_counts, n_actions\n",
    "    \n",
    "    def sampleTuples(self):\n",
    "        \n",
    "        # Get a batch of unprioritized samples:\n",
    "        \n",
    "        ids = list(np.random.choice(np.arange(self.n_samples), self.batch_size, replace=False))\n",
    "        batch = {}\n",
    "        for k in self.training_set.keys():\n",
    "            batch[k] = np.asarray(self.training_set[k], dtype=object)[ids]\n",
    "        batch['r'] = np.dot(batch['r'] * [1, 1, 10, 10, 100], self.reward_weights)\n",
    "        batch['s_ids'] = np.asarray(ids, dtype=int)\n",
    "        batch['ns_ids'] = np.asarray(ids, dtype=int) + 1\n",
    "            \n",
    "    \n",
    "        return batch\n",
    "    \n",
    "    def fitQ(self, batch, Q):\n",
    "        \n",
    "        # input = [state action]\n",
    "        x =  np.hstack((batch['s'], batch['a']))\n",
    "        \n",
    "        # target = r + gamma * max_a(Q(s', a))      == r for first iteration\n",
    "        y = batch['r'] + (self.gamma * np.max(Q[batch['ns_ids'], :], axis=1))\n",
    "        \n",
    "        self.q_est.fit(x, y)   \n",
    "    \n",
    "    def updateQtable(self, Qtable, batch):\n",
    "        \n",
    "        for i, a in enumerate(self.unique_actions):\n",
    "            #print(a, i)\n",
    "            Qtable[batch['s_ids'], i] = self.q_est.predict(np.hstack((batch['ns'], np.tile(a, (self.batch_size,1)))))\n",
    "        return Qtable\n",
    "    \n",
    "    def runFQI(self, repeats=10):\n",
    "        \n",
    "        print('Learning policy')\n",
    "        meanQtable = np.zeros((self.n_samples + 1, self.n_actions))\n",
    "        \n",
    "        for r in range(repeats):\n",
    "            print('Run', r, ':')\n",
    "            print('Initialize: get batch, set initial Q')\n",
    "            Qtable = np.zeros((self.n_samples + 1, self.n_actions))\n",
    "            Qdist = []\n",
    "\n",
    "            #print('Run FQI')\n",
    "            for iteration in range(self.iters):\n",
    "\n",
    "                # copy q-table\n",
    "                Qold = cp.deepcopy(Qtable)\n",
    "\n",
    "                # sample batch  \n",
    "                batch = self.sampleTuples()\n",
    "\n",
    "                # learn q_est with samples, targets from batch\n",
    "                self.fitQ(batch, Qtable)\n",
    "\n",
    "                # update Q table for all s given new estimator\n",
    "                self.updateQtable(Qtable, batch)\n",
    "\n",
    "                # check divergence from last estimate\n",
    "                Qdist.append(mean_absolute_error(Qold, Qtable))\n",
    "         \n",
    "            #plt.plot(Qdist)\n",
    "            meanQtable += Qtable\n",
    "        \n",
    "        meanQtable = meanQtable / repeats\n",
    "        print('Learn policy')\n",
    "        self.getPi(meanQtable)\n",
    "        return Qdist\n",
    "                    \n",
    "    \n",
    "    def getPi(self, Qtable):\n",
    "        optA = np.argmax(Qtable, axis=1)\n",
    "        print(\"Opta: \", optA)\n",
    "        #print(\"Fitting to training set\")\n",
    "        #print(\"Optimal actions: \", optA)\n",
    "        self.piE.fit(self.training_set['s'], optA[:-1])\n",
    "        #print(\"Done Fitting\")\n",
    "    \n",
    "    def testPi(self, behavior):\n",
    "        accurate = 0\n",
    "        total = 0\n",
    "        \n",
    "        for tup in self.raw_test:\n",
    "            s = tup[0]\n",
    "            try:\n",
    "                a = tup[1]\n",
    "                a = np.concatenate(a).ravel()\n",
    "                a = list(a)\n",
    "            except:\n",
    "                a = tup[1]\n",
    "            # actions based on policy we learn\n",
    "            s = s.reshape((1, 10))\n",
    "            evalA = self.piE.predict(s)\n",
    "            \n",
    "            # predicted actions based on historical actions model\n",
    "            behavB = behavior.predict(s)\n",
    "            \n",
    "            if behavB <= 0.25:\n",
    "                behavB = 0\n",
    "            elif behavB <= 0.5:\n",
    "                behavB = 1\n",
    "            elif behavB <= 0.75:\n",
    "                behavB = 2\n",
    "            else:\n",
    "                behavB = 3\n",
    "            \n",
    "            # actual historical actions\n",
    "            actions = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "            behavA = actions.index(a)\n",
    "            \n",
    "            if behavA == behavB:\n",
    "                accurate += 1\n",
    "            total += 1\n",
    "        \n",
    "        return float(accurate)/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fqi_agent = FQIagent(train_tuples=train_tuples, test_tuples=test_tuples)\n",
    "Q_dist = fqi_agent.runFQI(repeats=1)\n",
    "plt.plot(Q_dist, label= \"Vanilla FQI\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Q Estimate\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CFQI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class CFQIagent():\n",
    "    def __init__(self, train_tuples, test_tuples, iters=150, gamma=0.99, batch_size=100, prioritize=False, estimator='lin',\n",
    "                 weights=np.array([1, 1, 1, 1, 1])/5., maxT=36):\n",
    "        \n",
    "        self.iters = iters\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.prioritize_a = prioritize\n",
    "        self.training_set, self.test_set = construct_dicts(train_tuples, test_tuples)\n",
    "        self.raw_test = test_tuples\n",
    "        \n",
    "        self.visits = {'train': len(train_tuples), 'test': len(test_tuples)}\n",
    "        self.NV = {'train': len(train_tuples), 'test': len(test_tuples)}\n",
    "        self.n_samples = len(self.training_set['s'])\n",
    "        _, self.unique_actions, self.action_counts, _ = self.sub_actions()\n",
    "        self.state_feats = [str(x) for x in range(10)]\n",
    "        self.n_features = len(self.state_feats)\n",
    "        self.reward_weights = weights\n",
    "        self.maxT = maxT\n",
    "        self.piB = learnBehaviour(self.training_set, self.test_set)\n",
    "        self.n_actions = 4\n",
    "        \n",
    "        if estimator == 'tree':\n",
    "            self.q_est = ExtraTreesRegressor(n_estimators=50, max_depth=None, min_samples_leaf=10, min_samples_split=2,\n",
    "                                             random_state=0)\n",
    "        elif estimator == 'gbm':\n",
    "            self.q_est = LGBMRegressor(n_estimators=50, silent=True)\n",
    "\n",
    "        elif estimator == 'nn':\n",
    "            self.q_est = None\n",
    "        \n",
    "        elif estimator == 'lin':\n",
    "            self.q_est_shared = LinearRegression()\n",
    "            self.q_est_fg = LinearRegression()\n",
    "            \n",
    "        self.piE = LinearRegression()#LGBMClassifier(n_estimators=50, silent=True)\n",
    "        \n",
    "        self.eval_est = LGBMRegressor(n_estimators=50, silent=True)\n",
    "\n",
    "    def sub_actions(self):\n",
    "        \n",
    "        a = self.training_set['a']\n",
    "        a = list(a)\n",
    "        \n",
    "        unique_actions = 0\n",
    "        action_counts = 0\n",
    "        n_actions = 0\n",
    "        \n",
    "        unique_actions, action_counts = np.unique(a, axis=0, return_counts=True)\n",
    "        n_actions = len(unique_actions)\n",
    "                \n",
    "        return a, unique_actions, action_counts, n_actions\n",
    "    \n",
    "    def sampleTuples(self):\n",
    "        ids = list(np.random.choice(np.arange(self.n_samples), self.batch_size, replace=False))\n",
    "        batch = {}\n",
    "        for k in self.training_set.keys():\n",
    "            batch[k] = np.asarray(self.training_set[k], dtype=object)[ids]\n",
    "        batch['r'] = np.dot(batch['r'] * [1, 1, 10, 10, 100], self.reward_weights)\n",
    "        batch['s_ids'] = np.asarray(ids, dtype=int)\n",
    "        batch['ns_ids'] = np.asarray(ids, dtype=int) + 1\n",
    "            \n",
    "    \n",
    "        return batch\n",
    "    \n",
    "    def fitQ(self, batch, Q):\n",
    "        \n",
    "        # Divide into foreground and background batches. \n",
    "        batch_foreground = {}\n",
    "        batch_background = {}\n",
    "        \n",
    "        elts = ['s', 'a', 'ns', 'r', 'ds', 'vnum', 's_ids', 'ns_ids']\n",
    "        for el in elts:\n",
    "            batch_foreground[el] = []\n",
    "            batch_background[el] = []\n",
    "        \n",
    "        for i in range(len(batch['s_ids'])):\n",
    "            if batch['ds'][i] == 'foreground':\n",
    "                for k in batch.keys():\n",
    "                    batch_foreground[k].append(batch[k][i])\n",
    "            else:\n",
    "                for k in batch.keys():\n",
    "                    batch_background[k].append(batch[k][i])\n",
    "            \n",
    "        # input = [state action]\n",
    "        x_fg =  np.hstack((batch_foreground['s'], batch_foreground['a']))\n",
    "        x_shared = np.hstack((batch['s'], batch['a'])) \n",
    "        \n",
    "        # target = r + gamma * max_a(Q(s', a))      == r for first iteration\n",
    "        y_fg = batch_foreground['r'] + (self.gamma * np.max(Q[batch_foreground['ns_ids'], :], axis=1))\n",
    "        y_shared = batch['r'] + (self.gamma * np.max(Q[batch['ns_ids'], :], axis=1))\n",
    "        \n",
    "        # Used mixed model here\n",
    "        self.q_est_shared.fit(x_shared, y_shared)\n",
    "        self.q_est_fg.fit(x_fg, y_fg)\n",
    "        \n",
    "        return batch_foreground, batch_background\n",
    "    \n",
    "    def updateQtable(self, Qtable, batch_fg, batch_bg):\n",
    "        # Update for foregound using just foreground\n",
    "        # Update for background using shared\n",
    "        \n",
    "        bg_size = len(batch_bg['s'])\n",
    "        fg_size = len(batch_fg['s'])\n",
    "        for i, a in enumerate(self.unique_actions):\n",
    "            Qtable[batch_bg['s_ids'], i] = self.q_est_shared.predict(np.hstack((batch_bg['ns'], np.tile(a, (bg_size,1)))))\n",
    "            Qtable[batch_fg['s_ids'], i] = self.q_est_fg.predict(np.hstack((batch_fg['ns'], np.tile(a, (fg_size,1)))))\n",
    "        return Qtable\n",
    "    \n",
    "    def runFQI(self, repeats=10):\n",
    "        \n",
    "        print('Learning policy')\n",
    "        meanQtable = np.zeros((self.n_samples + 1, self.n_actions))\n",
    "        \n",
    "        for r in range(repeats):\n",
    "            print('Run', r, ':')\n",
    "            print('Initialize: get batch, set initial Q')\n",
    "            Qtable = np.zeros((self.n_samples + 1, self.n_actions))\n",
    "            Qdist = []\n",
    "\n",
    "            #print('Run FQI')\n",
    "            for iteration in range(self.iters):\n",
    "\n",
    "                # copy q-table\n",
    "                Qold = cp.deepcopy(Qtable)\n",
    "\n",
    "                # sample batch  \n",
    "                batch = self.sampleTuples()\n",
    "\n",
    "                # learn q_est with samples, targets from batch\n",
    "                batch_foreground, batch_background = self.fitQ(batch, Qtable)\n",
    "\n",
    "                # update Q table for all s given new estimator\n",
    "                self.updateQtable(Qtable, batch_foreground, batch_background)\n",
    "\n",
    "                # check divergence from last estimate\n",
    "                Qdist.append(mean_absolute_error(Qold, Qtable))\n",
    "         \n",
    "            #plt.plot(Qdist)\n",
    "            meanQtable += Qtable\n",
    "        \n",
    "        meanQtable = meanQtable / repeats\n",
    "        print('Learn policy')\n",
    "        \n",
    "        # Since the Q table is constructed contrastively, the policy is contrastive?\n",
    "        self.getPi(meanQtable)\n",
    "        return Qdist\n",
    "                    \n",
    "    \n",
    "    def getPi(self, Qtable):\n",
    "        optA = np.argmax(Qtable, axis=1)\n",
    "        print(\"Opta: \", optA)\n",
    "        #print(\"Fitting to training set\")\n",
    "        #print(\"Optimal actions: \", optA)\n",
    "        self.piE.fit(self.training_set['s'], optA[:-1])\n",
    "        #print(\"Done Fitting\")\n",
    "    \n",
    "    def testPi(self, behavior):\n",
    "        accurate = 0\n",
    "        total = 0\n",
    "        \n",
    "        for tup in self.raw_test:\n",
    "            s = tup[0]\n",
    "            try:\n",
    "                a = tup[1]\n",
    "                a = np.concatenate(a).ravel()\n",
    "                a = list(a)\n",
    "            except:\n",
    "                a = tup[1]\n",
    "            # actions based on policy we learn\n",
    "            s = s.reshape((1, 10))\n",
    "            evalA = self.piE.predict(s)\n",
    "            \n",
    "            # predicted actions based on historical actions model\n",
    "            behavB = behavior.predict(s)\n",
    "            \n",
    "            if behavB <= 0.25:\n",
    "                behavB = 0\n",
    "            elif behavB <= 0.5:\n",
    "                behavB = 1\n",
    "            elif behavB <= 0.75:\n",
    "                behavB = 2\n",
    "            else:\n",
    "                behavB = 3\n",
    "            \n",
    "            # actual historical actions\n",
    "            actions = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "            behavA = actions.index(a)\n",
    "            \n",
    "            if behavA == behavB:\n",
    "                accurate += 1\n",
    "            total += 1\n",
    "        \n",
    "        return float(accurate)/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfqi_agent = CFQIagent(train_tuples=train_tuples, test_tuples=test_tuples)\n",
    "Q_dist = cfqi_agent.runFQI(repeats=1)\n",
    "plt.plot(Q_dist, label= \"Contrastive FQI\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Q Estimate\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfqi_agent.q_est_shared.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfqi_agent.q_est_fg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed Model CFQI\n",
    "* Use a mixed model in the reward function stage and in the policy generation stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'num_legs': [2, 0, 1, 1, 2, 2, 2, 2],\n",
    "                   'num_wings': [2, 0, 0, 0, 1, 2, 3, 4],\n",
    "                   'num_specimen_seen': [10, 2, 1, 8, 1, 2, 3, 4]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outcome ~ Covariate\n",
    "# In our case, 12 covariates\n",
    "endog = df['num_wings'].to_numpy()\n",
    "exog = df['num_specimen_seen'].to_numpy()\n",
    "exog_re = np.asarray(exog.copy())\n",
    "groups = df['num_legs'].to_numpy()\n",
    "mm = sm.MixedLM.from_formula(\"num_wings ~ num_specimen_seen\", df, re_formula=\"num_specimen_seen\", groups=df['num_legs'])\n",
    "#mm = sm.regression.mixed_linear_model.MixedLM(endog=endog, exog=exog, groups=groups, exog_re=exog_re)\n",
    "\n",
    "\n",
    "mdf = mm.fit()\n",
    "\n",
    "print(mdf.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf.predict([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class MMFQIagent():\n",
    "    def __init__(self, train_tuples, test_tuples, iters=150, gamma=0.99, batch_size=100, prioritize=False, estimator='lin',\n",
    "                 weights=np.array([1, 1, 1, 1, 1])/5., maxT=36):\n",
    "        \n",
    "        self.iters = iters\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.prioritize_a = prioritize\n",
    "        self.training_set, self.test_set = construct_dicts(train_tuples, test_tuples)\n",
    "        self.raw_test = test_tuples\n",
    "        \n",
    "        self.visits = {'train': len(train_tuples), 'test': len(test_tuples)}\n",
    "        self.NV = {'train': len(train_tuples), 'test': len(test_tuples)}\n",
    "        self.n_samples = len(self.training_set['s'])\n",
    "        _, self.unique_actions, self.action_counts, _ = self.sub_actions()\n",
    "        self.state_feats = [str(x) for x in range(10)]\n",
    "        self.n_features = len(self.state_feats)\n",
    "        self.reward_weights = weights\n",
    "        self.maxT = maxT\n",
    "        self.piB = learnBehaviour(self.training_set, self.test_set)\n",
    "        self.n_actions = 4\n",
    "        \n",
    "        if estimator == 'tree':\n",
    "            self.q_est = ExtraTreesRegressor(n_estimators=50, max_depth=None, min_samples_leaf=10, min_samples_split=2,\n",
    "                                             random_state=0)\n",
    "        elif estimator == 'gbm':\n",
    "            self.q_est = LGBMRegressor(n_estimators=50, silent=True)\n",
    "\n",
    "        elif estimator == 'nn':\n",
    "            self.q_est = None\n",
    "        \n",
    "        elif estimator == 'lin':\n",
    "            self.q_est_shared = LinearRegression()\n",
    "            self.q_est_fg = LinearRegression()\n",
    "            \n",
    "        self.piE = LinearRegression()#LGBMClassifier(n_estimators=50, silent=True)\n",
    "        \n",
    "        self.eval_est = LGBMRegressor(n_estimators=50, silent=True)\n",
    "\n",
    "    def sub_actions(self):\n",
    "        \n",
    "        a = self.training_set['a']\n",
    "        a = list(a)\n",
    "        \n",
    "        unique_actions = 0\n",
    "        action_counts = 0\n",
    "        n_actions = 0\n",
    "        \n",
    "        unique_actions, action_counts = np.unique(a, axis=0, return_counts=True)\n",
    "        n_actions = len(unique_actions)\n",
    "                \n",
    "        return a, unique_actions, action_counts, n_actions\n",
    "    \n",
    "    def sampleTuples(self):\n",
    "        ids = list(np.random.choice(np.arange(self.n_samples), self.batch_size, replace=False))\n",
    "        batch = {}\n",
    "        for k in self.training_set.keys():\n",
    "            batch[k] = np.asarray(self.training_set[k], dtype=object)[ids]\n",
    "        batch['r'] = np.dot(batch['r'] * [1, 1, 10, 10, 100], self.reward_weights)\n",
    "        batch['s_ids'] = np.asarray(ids, dtype=int)\n",
    "        batch['ns_ids'] = np.asarray(ids, dtype=int) + 1\n",
    "            \n",
    "    \n",
    "        return batch\n",
    "    \n",
    "    def fitQ(self, batch, Q):\n",
    "        \n",
    "        # Divide into foreground and background batches. \n",
    "        batch_foreground = {}\n",
    "        batch_background = {}\n",
    "        \n",
    "        elts = ['s', 'a', 'ns', 'r', 'ds', 'vnum', 's_ids', 'ns_ids']\n",
    "        for el in elts:\n",
    "            batch_foreground[el] = []\n",
    "            batch_background[el] = []\n",
    "        \n",
    "        for i in range(len(batch['s_ids'])):\n",
    "            if batch['ds'][i] == 'foreground':\n",
    "                for k in batch.keys():\n",
    "                    batch_foreground[k].append(batch[k][i])\n",
    "            else:\n",
    "                for k in batch.keys():\n",
    "                    batch_background[k].append(batch[k][i])\n",
    "            \n",
    "        # input = [state action]\n",
    "        x_fg =  np.hstack((batch_foreground['s'], batch_foreground['a']))\n",
    "        x_shared = np.hstack((batch['s'], batch['a'])) \n",
    "        \n",
    "        # target = r + gamma * max_a(Q(s', a))      == r for first iteration\n",
    "        y_fg = batch_foreground['r'] + (self.gamma * np.max(Q[batch_foreground['ns_ids'], :], axis=1))\n",
    "        y_shared = batch['r'] + (self.gamma * np.max(Q[batch['ns_ids'], :], axis=1))\n",
    "        \n",
    "        self.q_est_shared.fit(x_shared, y_shared)\n",
    "        self.q_est_fg.fit(x_fg, y_fg)\n",
    "        \n",
    "        return batch_foreground, batch_background\n",
    "    \n",
    "    def updateQtable(self, Qtable, batch_fg, batch_bg):\n",
    "        # Update for foregound using just foreground\n",
    "        # Update for background using shared\n",
    "        \n",
    "        bg_size = len(batch_bg['s'])\n",
    "        fg_size = len(batch_fg['s'])\n",
    "        for i, a in enumerate(self.unique_actions):\n",
    "            Qtable[batch_bg['s_ids'], i] = self.q_est_shared.predict(np.hstack((batch_bg['ns'], np.tile(a, (bg_size,1)))))\n",
    "            Qtable[batch_fg['s_ids'], i] = self.q_est_fg.predict(np.hstack((batch_fg['ns'], np.tile(a, (fg_size,1)))))\n",
    "        return Qtable\n",
    "    \n",
    "    def runFQI(self, repeats=10):\n",
    "        \n",
    "        print('Learning policy')\n",
    "        meanQtable = np.zeros((self.n_samples + 1, self.n_actions))\n",
    "        \n",
    "        for r in range(repeats):\n",
    "            print('Run', r, ':')\n",
    "            print('Initialize: get batch, set initial Q')\n",
    "            Qtable = np.zeros((self.n_samples + 1, self.n_actions))\n",
    "            Qdist = []\n",
    "\n",
    "            #print('Run FQI')\n",
    "            for iteration in range(self.iters):\n",
    "\n",
    "                # copy q-table\n",
    "                Qold = cp.deepcopy(Qtable)\n",
    "\n",
    "                # sample batch  \n",
    "                batch = self.sampleTuples()\n",
    "\n",
    "                # learn q_est with samples, targets from batch\n",
    "                batch_foreground, batch_background = self.fitQ(batch, Qtable)\n",
    "\n",
    "                # update Q table for all s given new estimator\n",
    "                self.updateQtable(Qtable, batch_foreground, batch_background)\n",
    "\n",
    "                # check divergence from last estimate\n",
    "                Qdist.append(mean_absolute_error(Qold, Qtable))\n",
    "         \n",
    "            #plt.plot(Qdist)\n",
    "            meanQtable += Qtable\n",
    "        \n",
    "        meanQtable = meanQtable / repeats\n",
    "        print('Learn policy')\n",
    "        \n",
    "        # Since the Q table is constructed contrastively, the policy is contrastive?\n",
    "        self.getPi(meanQtable)\n",
    "        return Qdist\n",
    "                    \n",
    "    \n",
    "    def getPi(self, Qtable):\n",
    "        optA = np.argmax(Qtable, axis=1)\n",
    "        print(\"Opta: \", optA)\n",
    "        #print(\"Fitting to training set\")\n",
    "        #print(\"Optimal actions: \", optA)\n",
    "        self.piE.fit(self.training_set['s'], optA[:-1])\n",
    "        #print(\"Done Fitting\")\n",
    "    \n",
    "    def testPi(self, behavior):\n",
    "        accurate = 0\n",
    "        total = 0\n",
    "        \n",
    "        for tup in self.raw_test:\n",
    "            s = tup[0]\n",
    "            try:\n",
    "                a = tup[1]\n",
    "                a = np.concatenate(a).ravel()\n",
    "                a = list(a)\n",
    "            except:\n",
    "                a = tup[1]\n",
    "            # actions based on policy we learn\n",
    "            s = s.reshape((1, 10))\n",
    "            evalA = self.piE.predict(s)\n",
    "            \n",
    "            # predicted actions based on historical actions model\n",
    "            behavB = behavior.predict(s)\n",
    "            \n",
    "            if behavB <= 0.25:\n",
    "                behavB = 0\n",
    "            elif behavB <= 0.5:\n",
    "                behavB = 1\n",
    "            elif behavB <= 0.75:\n",
    "                behavB = 2\n",
    "            else:\n",
    "                behavB = 3\n",
    "            \n",
    "            # actual historical actions\n",
    "            actions = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "            behavA = actions.index(a)\n",
    "            \n",
    "            if behavA == behavB:\n",
    "                accurate += 1\n",
    "            total += 1\n",
    "        \n",
    "        return float(accurate)/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmfqi_agent = MMFQIagent(train_tuples=train_tuples, test_tuples=test_tuples)\n",
    "Q_dist = mmfqi_agent.runFQI(repeats=1)\n",
    "plt.plot(Q_dist, label= \"Mixed Model FQI\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Q Estimate\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation (FQI, CFQI, Oracle, Random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def cumulative_reward(rewards):\n",
    "    c_reward = [rewards[0]]\n",
    "    for i in range(1, len(rewards)):\n",
    "        c_reward.append(rewards[i] + c_reward[i-1])\n",
    "    return c_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Test out on each test tuple\n",
    "# FQI, CFQI, Oracle, Random\n",
    "algos = ['fqi', 'cfqi', 'oracle', 'random']\n",
    "overall_reward = {}\n",
    "for alg in algos:\n",
    "    overall_reward[alg] = []\n",
    "for k, pat in enumerate(tqdm.tqdm(range(num_patients))):\n",
    "    \n",
    "    flip = np.random.choice(2)\n",
    "    if flip == 0:\n",
    "        ds = 'foreground'\n",
    "    else:\n",
    "        ds = 'background'\n",
    "    # Generate a random initial state\n",
    "    s = np.random.normal(mu, sigma, (10, 1))\n",
    "    \n",
    "    val_rewards = {}\n",
    "    for alg in algos:\n",
    "        val_rewards[alg] = []\n",
    "    \n",
    "    \n",
    "    # Generate all of the tuples for this patient\n",
    "    for i in range(num_samples):\n",
    "        s = s.T\n",
    "        # FQI agent\n",
    "        fqi_action = fqi_agent.piE.predict(s)\n",
    "        if fqi_action[0] > 3:\n",
    "            fqi_action[0] = 3\n",
    "        fqi_action = actions[round(fqi_action[0])]\n",
    "        fqi_action = np.reshape(fqi_action, (2, 1))\n",
    "        s_a = np.concatenate((s.T, fqi_action))\n",
    "        val_rewards['fqi'].append(np.dot(reward_function.T, s_a)[0])\n",
    "\n",
    "\n",
    "        # CFQI agent\n",
    "        cfqi_action = cfqi_agent.piE.predict(s)\n",
    "        if cfqi_action[0] > 3:\n",
    "            cfqi_action[0] = 3\n",
    "        cfqi_action = actions[round(cfqi_action[0])]\n",
    "        cfqi_action = np.reshape(cfqi_action, (2, 1))\n",
    "        s_a = np.concatenate((s.T, cfqi_action))\n",
    "        val_rewards['cfqi'].append(np.dot(reward_function.T, s_a)[0])\n",
    "\n",
    "\n",
    "        # Oracle\n",
    "        all_rewards = []\n",
    "        for j, a in enumerate(actions):\n",
    "            a = np.asarray(a)\n",
    "            a = np.reshape(a, (2, 1))\n",
    "            s_a = np.concatenate((s.T, a))\n",
    "            reward = np.dot(reward_function.T, s_a)\n",
    "            all_rewards.append(reward)\n",
    "\n",
    "        all_rewards = np.asarray(all_rewards)\n",
    "        oracle_action = actions[np.argmax(all_rewards)]\n",
    "        val_rewards['oracle'].append(np.max(all_rewards))\n",
    "\n",
    "\n",
    "        # Random action\n",
    "        random_action = np.asarray(actions[np.random.choice(3)])\n",
    "        random_action = np.reshape(random_action, (2, 1))\n",
    "        s_a = np.concatenate((s.T, random_action))\n",
    "        val_rewards['random'].append(np.dot(reward_function.T, s_a)[0])\n",
    "        \n",
    "        if ds == 'foreground':\n",
    "            t_m = transition_foreground\n",
    "        else:\n",
    "            t_m = transition_background\n",
    "        ns = np.matmul(s_a.T, t_m) / np.linalg.norm(np.matmul(s_a.T, t_m), ord=2)\n",
    "        ns = np.add(ns, np.random.normal(0, 0.5, (1, 10))) # Add noise\n",
    "        s = ns.T\n",
    "    \n",
    "    plt.title(\"Rewards for trajectory: \" + str(k))\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Cumulative Reward\")\n",
    "    x = [i for i in range(num_samples)]\n",
    "    rewards_fqi = cumulative_reward(val_rewards['fqi'])\n",
    "    overall_reward['fqi'].append(rewards_fqi[-1])\n",
    "    rewards_cfqi = cumulative_reward(val_rewards['cfqi'])\n",
    "    overall_reward['cfqi'].append(rewards_cfqi[-1])\n",
    "    rewards_oracle = cumulative_reward(val_rewards['oracle'])\n",
    "    overall_reward['oracle'].append(rewards_oracle[-1])\n",
    "    rewards_random = cumulative_reward(val_rewards['random'])\n",
    "    overall_reward['random'].append(rewards_random[-1])\n",
    "    \n",
    "    \n",
    "    plt.plot(x, rewards_fqi, label=\"FQI\")\n",
    "    plt.plot(x, rewards_cfqi, label='CFQI')\n",
    "    plt.plot(x, rewards_oracle, label='Oracle')\n",
    "    plt.plot(x, rewards_random, label='Random')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Cumulative Reward across patients and algorithms\")\n",
    "sns.stripplot(overall_reward['fqi'], color='r', label='FQI')\n",
    "sns.stripplot(overall_reward['cfqi'], color='g', label='CFQI')\n",
    "sns.stripplot(overall_reward['random'], color='b', label='Random')\n",
    "sns.stripplot(overall_reward['oracle'], color='m', label=\"Oracle\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Cumulative Reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_fqi = np.asarray(overall_reward['fqi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_, axs = plt.subplots(2, 2)\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "#sns.set_theme(style=\"whitegrid\")\n",
    "plt.boxplot(x=[np.asarray(overall_reward['fqi']).squeeze().tolist(), np.asarray(overall_reward['cfqi']).squeeze().tolist(), np.asarray(overall_reward['random']).squeeze().tolist(), np.asarray(overall_reward['oracle']).squeeze().tolist()])# axs[0, 1].boxplot(x=np.asarray(overall_reward['cfqi']).squeeze().tolist())\n",
    "# axs[1, 0].set_title(\"Random\")\n",
    "# axs[1, 0].boxplot(x=np.asarray(overall_reward['random']).squeeze().tolist())\n",
    "# axs[1, 1].set_title(\"Oracle\")\n",
    "# axs[1, 1].boxplot(x=np.asarray(overall_reward['oracle']).squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
